distribut partial observ markov decis problem distribut pomdp popular approach multi-ag system uncertain domain signific complex pomdp particularli number agent popular approach approxim solut approach effici algorithm approach not guarante solut qualiti second popular approach focus global optim typic result avail onli agent also consider comput cost paper limit approach spider novel combin key featur polici gener distribut pomdp i agent interact structur network agent ie scale-up number agent ii combin heurist polici search iii qualiti approxim systemat tradeoff solut qualiti time experiment result order magnitud improv perform previou global optim algorithm categori subject descriptor i211 [ artifici intellig ] artifici intelligencemulti-ag system gener term algorithm theori introduct distribut partial observ markov decis problem distribut pomdp popular approach sequenti decis make team uncertainti [ ] uncertainti account nondetermin outcom action world state onli partial incorrectli observ unfortun bernstein al ] problem optim joint polici gener pomdp nexp-complet research differ type approach toward model first categori highli effici approxim techniqu not global optim solut ] key problem techniqu inabl guarante qualiti solut contrast second popular categori approach global optim result ] approach optim solut typic onli agent furthermor structur interact agent henc sever respect scalabl agent problem approach approxim techniqu guarante qualiti solut network agent first basic spider search polici distribut environ algorithm key novel featur spider i branch bound heurist search techniqu mdp-base heurist function optim joint polici ii network structur agent agent depth first search df tree advantag independ differ branch df tree then enhanc effici basic spider algorithm guarante qualiti solut first enhanc abstract speedup not solut qualiti particular initi branch bound search abstract polici then polici second enhanc obtain solut qualiti input paramet toler valu differ optim solut third enhanc again search effici howev toler paramet percentag optim sensor network domain nair al ] domain repres import class problem network agent uncertain environ experi spider global optim approach goa ] onli known global optim algorithm demonstr experiment result agent furthermor abstract perform spider significantli optim solut final key featur spider approxim principl tradeoff run-tim versu solut qualiti 978-81-904262-7-5 rp c ifaama domain distribut sensor net distribut sensor network larg import class domain work paper set target problem certain type sensor network ] first [ ] figur specif problem instanc type consist sensor here sensor node direct north south east west figur target associ reward sensor area same area simultan figur target loc11 sensor1 need ‘ east need ‘ west simultan thu sensor coordin fashion independ target target s movement uncertain unaffect sensor agent area sensor observ fals posit fals neg sensor observ transit independ other s action observ receiv independ sensor2 s action agent cost target present not cost sensor observ uncertainti target uncertain transit distribut natur sensor node sensor net use domain pomdp model figur 3-chain sensor configur model network distribut pomdp nd-pomdp model [ ] domain such sensor network section tupl s a p ω o r b s = ×1≤i≤nsi × su set world state si refer set local state agent i su set unaffect state unaffect state refer part world state not agent action eg environment factor target locat agent = ×1≤i≤nai set joint action ai set action agent i nd-pomdp assum transit independ transit function p s = pu su su pi si su ai si = a1 joint action state s = s1 sn su s = s1 sn su state = ×1≤i≤nωi set joint observ ωi set observ agent i observ independ nd-pomdp ie joint observ function o ω oi si su ai ωi s = s1 sn su world state result agent = a1 previou state ω = ω1 ωn ∈ ω observ state s agent s observ onli unaffect state local action local state reward function r r = l rl sl1 slr su al1 alr l sub-group agent r = |l| reward function interact hypergraph hyper-link l subset agent rl r interact hypergraph g = ag e agent ag vertic e = l|l ⊆ ag ∧ rl compon r edg initi belief state distribut initi state b b s = bu su bi si bu bi refer distribut initi unaffect state agent i s initi belief state respect goal nd-pomdp joint polici π = π1 team s reward finit horizon t belief state b nd-pomdp similar n-ari distribut constraint optim problem dcop ] variabl node polici individu agent πi domain variabl set local polici πi reward compon rl |l| local constraint reward compon rl l correspond non-loc constraint constraint graph algorithm global optim algorithm goa previou work goa global optim algorithm nd-pomdp [ ] goa experiment comparison goa state-of-the-art global optim algorithm fact onli one experiment result avail network agent goa global optim dcop algorithm dpop [ ] goa s messag pass dpop first phase util propag util messag case polici leav root valu polici agent sum respons valu children joint polici reward parent polici thu polici parent node goa agent polici respons polici valu parent while parent node polici agent children respons polici util propag process level tree root polici second phase valu propag optim polici root leav goa advantag local interact interact graph unnecessari joint polici evalu node not directli tree interact graph reward interact agent algorithm relev joint polici evalu algorithm global optim solut spider section nd-pomdp dcop goal joint polici overal joint reward brute-forc techniqu optim polici expect valu possibl joint polici key idea spider comput valu entir space joint polici upper bound expect valu polici interact structur agent akin algorithm dcop [ ] spider pre-process step df tree interact structur note df tree pseudo tree ] link ancestor children maximum constrain node mcn heurist dcop algorithm adopt ] howev other heurist such mlsp heurist ] also mcn heurist tri agent number constraint top tree tree search optim joint polth sixth intl joint conf autonom agent multi-ag system aama ici proce spider algorithm paper easili extend hyper-tre howev expositori purpos binari tree spider algorithm central plan distribut execut distribut pomdp paper follow notat polici valu ancestor i ⇒ agent i root not i tree i ⇒ agent sub-tre not i i root ⇒ joint polici agent ⇒ joint polici agent tree i ∪ i πi− ⇒ joint polici agent ancestor i πi ⇒ polici ith agent [ πi πi− ] ⇒ upper bound valu πi+ πi polici ancestor agent πi− [ πi πi− ] ⇒ upper bound valu πi+ jth child v [ πi πi− ] ⇒ valu πi polici ancestor agent πi− v [ πi+ πi− ] ⇒ valu πi+ polici ancestor agent πi− vj [ πi+ πi− ] ⇒ valu πi+ jth child figur execut spider exampl outlin spider spider idea branch bound search node search tree repres partial/complet joint polici figur exampl search tree spider algorithm exampl agent chain spider search df tree ie tree agent chain middl agent root tree spider structur df tree search note exampl figur agent polici t=2 thu round rectang search tree node partial/complet joint polici rectangl agent oval intern agent show polici heurist actual valu joint polici top right corner round rectangl number actual valu joint polici spider polici agent level search tree search tree joint polici upper bound root agent s polici level spider search node complet joint polici polici agent expect valu joint polici node level one upper bound polici non-leaf agent i spider potenti step upper bound step agent i upper bound expect valu ˆv [ πi πi− ] joint polici polici πi fix ancestor polici mdp heurist upper bound expect valu descript mdp heurist section polici agent i then upper bound also heurist valu henceforth order explor polici step descend order level search tree figur joint polici heurist valu top right corner joint polici intuit then polici order upper bound polici upper bound joint polici valu explor prune explor impli respons joint polici πi+ ∗ ancestor polici agent i πi− polici agent i πi quantiti polici i respons i s children step child node ii valu i fix polici ancestor thu explor polici πi yield actual valu joint polici πi+ v [ πi+ πi− ] polici expect valu respons polici refer polici valu agent i current valu henceforth vmax [ πi+ πi− ] threshold polici πi not upper bound polici ˆv [ πi πi− ] threshold expect valu joint polici attain polici threshold other hand leaf agent spider respons polici consequ valu polici ancestor πi− valu polici polici ancestor expect valu polici figur spider respons polici agent level polici left leaf agent action east time step polici polici right leaf agent off time step respons polici leaf agent actual valu complet joint polici pseudo code spider algorithm joint polici πi+ ∗ valu threshold agent tree i line 3-8 comput respons polici leaf agent i line respons joint polici agent tree i respons comput non-leaf agent i sort polici order heurist valu line b respons polici children fix polici agent i line 16-20 c sixth intl joint conf autonom agent multi-ag system aama spider i πi− threshold πi+ ∗ ← null πi ← get-all-polici horizon ai ωi is-leaf i then πi ∈ πi v [ πi πi− ] ← joint-reward πi πi− v πi πi− ] > then πi+ ∗ ← threshold ← v [ πi πi− els children children i ˆπi ← upper-bound-sort i πi πi− πi ∈ ˆπi ˜πi+ ← ˆv [ πi πi− ] < then line j ∈ children jthre threshold − v [ πi πi− ] − σk∈children ˆvk [ πi πi− πj+ ∗ ← spider j πi πi− ˜πi+ ← ˜πi+ πj+ ˆvj [ πi πi− ] ← v [ πj+ ∗ πi πi− v ˜πi+ πi− ] > then threshold ← v [ ˜πi+ πi− πi+ ∗ ← return πi+ ∗ algorithm upper-bound-sort i πi πi− children children i ˆπi null / * sort list * πi ∈ πi ˆv [ πi πi− ] ← joint-reward πi πi− j ∈ children ˆvj [ πi πi− ] ← upper-bound i j πi πi− ˆv [ πi πi− ] + ← ˆvj [ πi πi− ˆπi ← insert-into-sort πi ˆπi return ˆπi valu joint polici line 21-23 pseudo code polici upper bound expect valu joint polici valu agent i part valu ancestor valu children comput valu ancestor agent joint-reward function line 5-7 heurist valu children sum part yield upper bound valu agent i line algorithm polici upper bound mdp heurist function heurist function quickli upper bound valu obtain agent tree i subtre agent distribut pomdp idea here central mdp correspond sub-tre pomdp expect valu optim polici central mdp term agent df tree interact structur full observ agent tree i fix polici agent ancestor i ∪ i joint valu ˆv [ πi+ πi− ] follow notat equat upper bounds/heurist valu agent i k ei− set link agent ancestor i ∪ i tree i ei+ set link agent tree i also l ∈ ei− then l1 agent ancestor i ∪ i l2 agent tree i l togeth first standard notat ot k =ok st+1 k st+1 u πk ωt k ωt+1 k pt k =pk st k st u πk ωt k st+1 k ot k pt u =p st u st+1 u st l = st l1 st l2 st u l = ωt l1 ωt rt l =rl st l πl1 ωt l1 πl2 ωt l2 l =v t πl st l st u ωt l1 ωt l2 locat agent k agent tree follow case if ∈ ancestor i ∪ i ˆpt k = pt k if ∈ tree i ˆpt k = pk st k st u πk ωt k st+1 k if l ∈ ei− ˆrt l = max al2 rl st l πl1 ωt l1 al2 if l ∈ ei+ ˆrt l = max al1 al2 rl st l al1 al2 valu function agent i joint polici πi+ time η − equat v η−1 πi+ sη−1 ωη−1 l∈ei− vη−1 l + l vη−1 l = rη−1 l + ω η l sη pη−1 l1 pη−1 l2 pη−1 u vη l algorithm upper-bound i j πj− val l ∈ ej− ∪ ej+ l ej− then ← φ s0 l val + ← [ s0 l ] · upper-bound-tim i s0 l j πl1 return val algorithm upper-bound-tim i st l j πl1 ωt l1 maxv al ← al1 al2 l ∈ ei− l ∈ ej− then ← πl1 ωt l1 val ← get-reward st l al1 al2 t πihorizon then st+1 l ωt+1 futv al←pt u ˆpt l1 ˆpt futv al ∗ ← upper-bound-tim st+1 l j πl1 ωt ωt+1 l1 val + ← al val > maxv al then al ← val return maxv upper bound valu link equat full observ assumpt observ probabl term agent tree i futur valu ˆvη l action agent tree i thu equat sixth intl joint conf autonom agent multi-ag system aama comput upper bound link l if ∈ ei− ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 s η l ˆpη−1 ˆpη−1 l2 pη−1 u ˆvη l if l ∈ ei+ ˆvη−1 l =ˆrη−1 l + max al1 al2 s η l ˆpη−1 ˆpη−1 l2 pη−1 u ˆvη l algorithm algorithm algorithm upper bound child j agent i equat upper bound link state algorithm upper bound valu link ei− ∪ ei+ abstract algorithm spider-ab i πi− threshold πi+ ∗ ← null πi ← get-polici < > is-leaf i then πi ∈ πi absheurist ← get-abs-heurist πi πi− absheurist ∗ ← timehorizon − πihorizon πihorizon = timehorizon πiabsnod then v [ πi πi− ] ← joint-reward πi πi− v πi πi− ] > then πi+ ∗ ← πi ← v [ πi πi− els v [ πi πi− ] + absheurist > then ˆπi ← extend-polici πi πiabsnod + πi ← insert-sorted-polici ˆπi remov πi els children children i πi ← upper-bound-sort i πi πi− πi ∈ πi ˜πi+ ← absheurist ← get-abs-heurist πi πi− absheurist ∗ ← timehorizon − πihorizon πihorizon = timehorizon πiabsnod then ˆv [ πi πi− ] < threshold πiabsnod then line j ∈ children jthre threshold − v [ πi πi− ] − σk∈children ˆvk [ πi πi− πj+ ∗ ← spider j πi πi− ˜πi+ ← ˜πi+ πj+ ∗ ˆvj [ πi πi− ] ← v [ πj+ ∗ πi πi− v ˜πi+ πi− ] > then threshold ← v [ ˜πi+ πi− ] πi+ ∗ ← els ˆv [ πi+ πi− ] + absheurist > then ˆπi ← extend-polici πi πiabsnod + πi ← insert-sorted-polici ˆπi remov πi return πi+ ∗ spider phase onli heurist bound comput polici approach possibl explor group polici heurist comput abstract polici thu improv runtim perform loss solut qualiti import step techniqu abstract polici heurist valu abstract polici paper type abstract horizon abstract hba here abstract polici horizon polici group horizon polici same action abstract polici time equal horizon abstract polici figur t=1 abstract polici east action group t=2 polici perform east first time step hba part heurist comput upper bound horizon abstract polici same heurist comput getheurist algorithm spider howev time horizon horizon abstract polici b maximum possibl reward time step get-abs-heurist number time step time horizon maximum possibl reward time step action agent tree i maximum joint reward joint action sum b heurist valu hba abstract polici node abstract nba here abstract polici not action certain node polici tree hba multipl level abstract figur b t=2 polici not action observ ‘ tp incomplet t=2 polici abstract t=2 complet polici level abstract lead comput complet joint polici πroot+ also heurist comput explor phase nba heurist comput similar normal polici case action polici node such case immedi reward rmax maximum reward action abstract techniqu techniqu spider-ab algorithm algorithm abstract techniqu optim joint polici spider-ab non-leaf agent i initi abstract t=1 polici line abstract polici heurist comput line abstract horizon gradual abstract polici then order heurist valu one heurist valu threshold line explor spider-ab same definit spider polici horizon polici comput equal actual time horizon node polici action line howev condit not then group polici extend-polici function line 31-32 extend-polici function also respons horizon absnod polici number node last level polici tree not action πiabsnod |ωi|πihorizon−1 ie total number polici node possibl πihorizon then πihorizon otherwis πiabsnod thu function hba nba polici variabl horizon absnod abstract polici group polici polici heurist valu line similar type abstract respons comput leaf agent line 3-14 valu approxim vax section approxim enhanc spider vax input techniqu approxim paramet differ optim solut qualiti approxim paramet agent joint polici mechan spider spider-ab joint polici onli threshold exactli heurist valu howev sixth intl joint conf autonom agent multi-ag system aama figur exampl abstract hba horizon base abstract b nba node base abstract idea techniqu joint polici follow condit satisfi threshold + > ˆv [ πi πi− ] apart condit vax same spider/spider-ab exampl figur heurist valu second joint polici second search tree node level instead then polici not spider spider-ab howev vax approxim paramet joint polici consider also threshold junctur approxim paramet ie heurist valu joint polici exampl just kind prune explor henc lead improv overal run-tim perform howev sacrific qualiti solut techniqu candid optim solut bound error approxim algorithm function proposit percentag approxim pax section second approxim enhanc spider pax input techniqu paramet minimum percentag optim solut qualiti output techniqu polici valu % optim solut qualiti polici follow condit satisfi threshold > δ ˆv [ πi πi− ] vax onli differ pax spider/spider-ab condit again figur heurist valu second search tree node level instead then pax input paramet % abl search tree node ∗238 < type lead explor henc improv run-tim perform potenti loss qualiti solut proposit bound qualiti loss theoret result proposit heurist central mdp heurist admiss proof valu heurist admiss over estim expect valu joint polici thu l ∈ ei+ ∪ ei− ˆvt l ≥ vt l notat section mathemat induct t base case t = t l ∈ ei− l ∈ ei+ ˆrt l action agent tree i rt l fix polici same agent henc ˆrt l ≥ rt l also ˆvt l ≥ vt l assumpt proposit t = η ≤ η < t now proposit t = η proof l ∈ ei− similar reason l ∈ ei+ heurist valu function l ∈ ei− follow equat ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 s η l ˆpη−1 ˆpη−1 l2 pη−1 u ˆvη l rh eqn section =ˆrη−1 l + max al2 ω η l1 s η l pη−1 u pη−1 l1 ˆpη−1 ˆvη l =ˆrη−1 l + ω η l1 s η l pη−1 u pη−1 l1 max al2 ˆpη−1 ˆvη l maxal2 ˆpη−1 ˆvη l ≥ ωl2 ˆpη−1 l2 ˆvη l pη−1 l2 = l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l ˆvη l ≥ vη l assumpt l + ω η l1 s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l ˆrη−1 l ≥ rη−1 l definit ≥rη−1 l + ω η l1 s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + ω η l s η l pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l thu proposit spider optim solut proof spider possibl joint polici interact structur agent onli except joint polici heurist valu thu as long candid optim polici not spider optim polici proposit expect valu joint polici alway upper bound henc joint polici not optim solut proposit error bound solut qualiti vax spider-ab approxim paramet ρ ρ number leaf node df tree sixth intl joint conf autonom agent multi-ag system aama proof proposit mathemat induct depth df tree base case depth ie node respons polici πk polici πk ˆv [ πk πk− ] < + respons polici vax away optim respons henc proposit base case assumpt proposit d ≤ depth ≤ now proposit d loss gener root node tree children children depth ≤ d henc assumpt error kth child ρk ρk number leaf node kth child root therefor ρ = k ρk ρ number leaf node tree spider-ab threshold root agent thresspid = k v [ πk+ πk− ] howev vax threshold root agent case threshvax = k v [ πk+ πk− ] − k ρk henc vax joint polici root agent ˆv [ πroot πroot− ] < threshvax + ⇒ ˆv [ πroot πroot− ] < threshspid − ρk ≤ threshspid − ρk threshspid − ρ henc proposit pax spider-ab input paramet δ solut qualiti v [ πroot+ ∗ ] v [ πroot+ ∗ ] optim solut qualiti proof proposit mathemat induct depth df tree base case depth ie node respons polici πk polici πk δ ˆv [ πk πk− ] < threshold respons polici pax time optim respons henc proposit base case assumpt proposit d ≤ depth ≤ now proposit d loss gener root node tree children children depth ≤ d henc assumpt solut qualiti kth child v [ πk+ ∗ πk− ] pax spider-ab joint polici root agent ˆv [ πroot πroot− ] < k v [ πk+ ∗ πk− ] howev pax joint polici δ ˆv [ πroot πroot− ] < δ v [ πk+ ∗ πk− ] ⇒ ˆv [ πroot πroot− ] < k v [ πk+ ∗ πk− ] condit root agent pax same spider-ab error root agent error children thu overal solut qualiti δ optim solut henc experiment result all experi sensor network domain section network configur figur goa spider spider-ab pax vax goa onli global optim algorithm agent set experi i firstli run-tim perform abov algorithm ii secondli pax vax tradeoff run-tim solut qualiti experi seconds1 figur run-tim comparison optim algorithm goa spider spider-ab approxim algorithm pax vax δ x-axi machin spec experi intel xeon ghz processor ram sensor network configur y-axi runtim log-scal time horizon polici comput configur 3-chain 4-chain 4-star 5-star bar time goa spider spiderab pax vax goa not time limit 4-star 5-star configur spider-ab spider goa configur instanc configur spider-ab 230-fold speedup goa 2-fold speedup spider 4-chain configur 58-fold speedup goa 2-fold speedup spider approxim approach vax pax improv perform spider-ab instanc 5-star configur vax 15-fold speedup pax 8-fold speedup spider-ab figur b comparison solut qualiti differ algorithm problem figur x-axi sensor network configur y-axi solut qualiti goa spider spider-ab global optim algorithm solut qualiti same algorithm 5-p configur global optim algorithm not limit second bar optim qualiti upper bound optim solut qualiti approxim solut qualiti close optim solut qualiti 3-chain 4-star configur remark pax vax almost same actual qualiti global optim algorithm approxim paramet δ other configur well loss qualiti % optim solut qualiti figur c time solut pax epsilon x-axi approxim paramet δ percentag y-axi time solut log-scal time horizon configur δ time solut drastic instanc 3-chain case total speedup 170-fold δ interestingli even low δ % actual solut qualiti equal one % figur d time configur vax epsilon x-axi approxim paramet y-axi time solut log-scal time horizon configur time solut drastic instanc 4-star case total speedup 73-fold again actual solut qualiti not epsilon figur sensor network configur sixth intl joint conf autonom agent multi-ag system aama figur comparison goa spider spider-ab vax t = runtim b solut qualiti time pax percentag t=4 d time solut vax epsilon t=4 summari and relat work paper algorithm spider spider-ab pax vax novel combin featur polici search distribut pomdp i agent interact structur network agent ie scale-up number agent ii branch bound search mdp heurist function iii abstract runtim perform solut qualiti iv priori percentag bound qualiti solut pax v valu qualiti solut vax featur systemat tradeoff solut qualiti run-tim network agent uncertainti experiment result order magnitud improv perform previou global optim algorithm research typic type techniqu pomdp first set techniqu global optim solut hansen al ] present algorithm dynam program iter elimin domin polici optim solut distribut pomdp szer al ] optim heurist search method decentr pomdp algorithm combin classic heurist search algorithm a∗ control theori key differ spider maa * enhanc spider vax pax provid qualiti approxim maa * global optim algorithm henc signific comput complex b due maa * s inabl interact structur onli agent howev spider network agent c spider joint polici agent time maa * time step time simultan agent second set techniqu approxim polici emerymontemerlo al ] approxim posg seri one-step bayesian game heurist futur valu limit lookahead comput effici local optim polici respect heurist nair al ] s jesp algorithm dynam local optimum solut finit horizon pomdp peshkin al ] bernstein al ] exampl polici search techniqu local optim polici abov techniqu effici polici comput consider unabl error bound qualiti solut aspect qualiti bound spider abov techniqu acknowledg materi work defens advanc research project agenc darpa depart interior nbc acquisit servic divis contract no nbchd030010 view conclus document author not offici polici either defens advanc research project agenc us govern 