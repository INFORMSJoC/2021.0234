new graphic represent interact partial observ markov decis process i-pomdp significantli transpar semant clear previou represent graphic model interact dynam influenc diagram i-did seek explicitli structur often present real-world problem situat chanc decis variabl depend variabl i-did gener did graphic represent pomdp set same way i-pomdp gener pomdp i-did polici agent onlin agent act observ set other agent sever exampl i-did use categori subject descriptor i211 [ distribut artifici intellig ] multiag system gener term theori introduct interact partial observ markov decis process ipomdp ] framework sequenti decision-mak partial observ multiag environ pomdp [ ] set other agent comput model state space state physic environ model inform agent behavior prefer capabl belief thu analog type bayesian game ] i-pomdp subject approach strateg behavior decision-theoret framework decision-mak s perspect interact [ ] polich gmytrasiewicz interact dynam influenc diagram i-did comput represent i-pomdp i-did gener did ] comput counterpart pomdp multiag set same way i-pomdp gener pomdp i-did contribut line work ] multi-ag influenc diagram maid ] recent network influenc diagram nid ] formal explicitli structur often present real-world problem situat chanc decis variabl depend variabl maid altern normal extens game form graphic formal game imperfect inform decis node agent s action chanc node agent s privat inform maid object game effici nash equilibrium profil independ structur nid maid agent uncertainti game model other agent model maid network maid singl maid equilibrium game mind differ model agent graphic formal such maid nid promis area research multiag interact transpar howev maid analysi game extern viewpoint applic static singl play game matter complex interact time predict other futur action model agent act observ i-did address gap represent other agent model valu special model node other agent model origin agent s model time special-purpos implement paper previou preliminari represent i-did [ ] insight static i-id type nid thu nid-specif languag construct such multiplex model node subsequ i-id transpar furthermor semant special purpos polici link represent i-did [ ] tradit depend link previou represent i-did updat agent s belief model other agent act receiv observ special link model updat link model time semant link tradit depend link chanc node model node net result represent i-did significantli transpar semant clear capabl standard algorithm did idid agent s uncertainti other model i-did solut i-did polici agent time belief physic state other model analog did i-did polici agent onlin agent act observ set other agent background finit nest ipomdp interact pomdp pomdp set other agent model part state space ] other agent also reason other interact state space strateg belief other agent model belief other simplic present agent i other agent j finit nest i-pomdp agent i strategi level l tupl i-pomdpi l = isi l a ti ωi oi ri • isi l set interact state isi l = s × mj l−1 mj = θj l−1 ∪ smj l ≥ isi,0 = s s set state physic environ θj l−1 set comput intent model agent j θj l−1 = bj l−1 frame ˆθj a ωj tj oj rj ocj here j bay ration ocj j s optim criterion smj set subintent model j simpl exampl subintent model no-inform model ] fictiti play model ] histori independ recurs bottom-up construct interact state space isi,0 = s θj,0 = bj,0 ˆθj | bj,0 ∈ δ isj,0 isi,1 = s × θj,0 ∪ smj θj,1 = bj,1 ˆθj | bj,1 ∈ δ isj,1 isi l = s × θj l−1 ∪ smj θj l = bj l ˆθj | bj l ∈ δ isj l similar formul nest space [ ] = ai × aj set joint action agent environ ti s ×a× → [ ] effect joint action physic state environ ωi set observ agent i oi s a × ωi → [ ] likelihood observ physic state joint action ri isi a → r agent i s prefer interact state usual onli physic state agent i s polici map i δ ai ω∗ i set observ histori agent i belief interact state suffici statist [ ] polici also map set belief agent i distribut action δ isi δ ai belief updat analog pomdp agent i-pomdp framework belief observ howev differ belief updat multiag set singl agent one first state physic environ action agent i s predict physic state chang predict j s action second chang j s model i s belief updat specif j intent then updat j s belief due action observ other word i belief predict j j belief j s model subintent then j s probabl observ observ histori model formal pr ist |at−1 i bt−1 i l β ist−1 mt−1 j =θt j bt−1 i l ist−1 at−1 j pr at−1 j |θt−1 j l−1 oi st at−1 i at−1 j i ×ti st−1 at−1 i at−1 j st j oj st at−1 i at−1 j ot j ×τ seθt j bt−1 j l−1 at−1 j ot j bt j l−1 β normal constant τ argument otherwis pr at−1 j |θt−1 j l−1 probabl j bay ration agent model θt−1 j l−1 se · abbrevi belief updat version belief updat j s model subintent [ ] agent j also i-pomdp then i s belief updat invok j s belief updat term seθt j bt−1 j l−1 at−1 j ot j turn i s belief updat so recurs belief nest bottom 0th level level belief updat agent pomdp belief updat illustr belief updat addit detail i-pomdp other multiag framework [ ] valu iter belief state finit i-pomdp valu maximum payoff agent belief state un bi l θi max ai∈ai is∈isi l eri ai bi l pr bi l un−1 seθi bi l ai oi θi eri ai = aj ri ai aj pr aj|mj l−1 = mj l−1 eq basi valu iter i-pomdp agent i s optim action a∗ i case finit horizon discount element set optim action belief state opt θi opt bi l θi argmax ai∈ai is∈isi l eri ai bi l oi∈ωi pr bi l un seθi bi l ai oi θi a naiv extens influenc diagram id set multipl agent possibl other agent automaton chanc node howev approach agent action probabl distribut not time interact influenc diagram i-id sophist approach id applic set other agent belief syntax addit usual chanc decis util node iid new type node model node gener level l i-id fig model node mj l−1 hexagon probabl distribut chanc node s model node togeth agent i s belief interact state addit model level model pomdp other agent s action exogen event t o r function sixth intl joint conf autonom agent multi-ag system aama figur gener level i-id agent i other agent j hexagon model node mj l−1 structur b member model node i-id m1 j l−1 j l−1 not here simplic decis node correspond chanc node a1 j a2 j valu node mod [ mj ] distribut chanc node node aj c i-id model node chanc node relationship node i-id differ id dash link polici link [ ] model node chanc node aj distribut other agent s action model absenc other agent model node chanc node aj vanish i-id collaps tradit id model node altern comput model i other agent set θj ∪ smj θj l−1 smj previous section thu model model node i-id id recurs model id subintent model node altern model other agent valu represent not trivial particular model node i-id solv agent s optim polici decis node decis node correspond chanc node a1 j follow way opt set optim action i-id id then pr aj ∈ a1 j |op t | aj ∈ opt otherwis insight previou work [ ] model node dash polici link chanc node aj fig b decis node level l i-id chanc node previous action valu decis node uniform probabl chanc node rest probabl differ chanc node a1 j a2 j model addit chanc node mod [ mj ] parent chanc node aj thu mani action node a1 j a2 j mj l−1 number model support agent i s belief condit probabl tabl chanc node aj multiplex distribut action node a1 j a2 j valu mod [ mj ] valu mod [ mj ] differ model j other word mod [ mj ] valu m1 j l−1 chanc node aj distribut node a1 j aj distribut a2 j mod [ mj ] valu m2 j l−1 distribut node mod [ mj ] agent i s belief model j physic state agent as mani model node agent notic fig b semant polici link tradit depend link fig c transform i-id model node chanc node relationship contrast represent ] special-purpos polici link rather i-id onli type node tradit id depend relationship node i-id convent applic tool id note level l i-id nid specif level l model model node block nid fig level l block tradit id otherwis > block nid nid note i-id id level onli singl decis node thu nid not maid figur level l i-id nid probabl block nid i s j s model physic state solut solut i-id proce bottom-up manner recurs level model intent tradit id solut probabl distribut other agent action correspond chanc node model node level i-id map level model decis node chanc node so action valu decis node uniform probabl chanc node rest probabl distribut action differ chanc node model other agent level i-id fig c transform condit probabl tabl cpt node aj such node distribut chanc node valu node mod [ mj ] previous valu node mod [ mj ] differ model other agent distribut agent i s belief model j physic state transform level i-id tradit id sixth intl joint conf autonom agent multi-ag system aama b figur gener time-slic level i-did agent i set other agent j notic dot model updat link updat model j distribut model time b semant model updat link standard util maxim method ] procedur level l i-id solut non-empti set optim action agent belief notic id i-id suitabl onlin decision-mak agent s current belief interact dynam influenc diagram interact dynam influenc diagram i-did i-id nid sequenti decision-mak sever time step just did structur graphic represent pomdp i-did graphic onlin analog finit i-pomdp i-did finit look-ahead initi belief other possibl similar agent syntax gener time-slic i-did fig addit model node dash polici link i-did did model updat link dot arrow fig semant model node polici link previou section model next updat model node time step first model time t updat set model model node time t section agent s intent model belief agent act receiv observ model belief set optim action model action agent |ωj| possibl observ updat set time step t |mt j l−1||aj||ωj| model here |mt j l−1| number model time step t |aj| |ωj| space action observ respect model second new distribut updat model origin distribut probabl agent action observ model step part agent i s belief updat eq fig b dot model updat link i-did level l model time step t result action j possibl observ then model node time step t contain updat model mt+1,1 j l−1 j l−1 j l−1 mt+1,4 j l−1 model initi belief result j belief due action possibl observ decis node i-did did level model correspond figur i-did model node model updat link chanc node relationship bold chanc node previous next distribut updat set model distribut chanc node mod [ mt+1 j ] mt+1 j l−1 probabl j s model mt+1,1 j l−1 probabl j action observ model prior distribut model time step t chanc node j distribut action node valu mod [ mt j ] probabl action chanc node order probabl j possibl observ chanc node oj valu mod [ mt j ] distribut observ node level model mod [ mt j ] probabl j s observ physic state joint action agent node oj st+1 j i analog j condit probabl tabl oj also multiplex mod [ mt j ] final distribut prior model time t chanc node mod [ mt j ] mt j l−1 consequ chanc node mod [ mt j ] j oj parent mod [ mt+1 j ] mt+1 j l−1 notic model updat link depend chanc node model node time slice fig time-slic i-did model node chanc node relationship chanc node depend link not bold standard usual did expans i-did time step repetit step set model note oj j s observ time t sixth intl joint conf autonom agent multi-ag system aama valu model node relationship chanc node mani time model updat link possibl set model other agent j exponenti number time step exampl t step |mt=1 j l−1| |aj||ωj| t −1 candid model model node solut analog i-id solut level l i-did agent i t time step recurs purpos illustr l=1 t=2 solut method standard look-ahead techniqu agent s action observ sequenc forward current belief state ] possibl belief i next time step agent i belief j s model well lookahead possibl model j futur consequ j s subintent level model standard did first time step optim set action action set possibl observ j model updat set candid model updat belief behavior j belief updat set candid model standard infer method depend relationship model node fig b recurs natur solut agent i level i-did j s level did nest model model level bottom-up manner recurs algorithm agent i s algorithm i-did input level l i-id level id t expans phase t t − l then popul mt+1 j mt j rang mt j l−1 recurs call algorithm l i-id id j horizon t − + decis node i-id id opt mt j chanc node aj aj opt mt j oj oj part mt j j s belief bt+1 j ← se bt j aj oj mt+1 j new i-id id bt+1 j initi belief rang mt+1 j l−1 ← mt+1 j model node mt+1 j l−1 depend mt j l−1 mt+1 j l−1 fig b chanc decis util node t time slice depend link cpt chanc node util node look-ahead phase standard look-ahead backup method expand i-did figur algorithm level l i-did level l i-did t time step other agent j fig two-phas approach i-id level l previous section level model also i-id id level first step level l i-id t time step depend link condit probabl tabl node particularli model node line 3-11 note rang · valu level model random variabl input node second phase standard look-ahead techniqu action observ sequenc t time step futur util valu reachabl belief similar i-id i-did did absenc other agent previous 0-th level model tradit did solut probabl distribut action agent level i-did level probabl distribut other agent s action level idid did probabl distribut yet level model number model level number m i-did level l then equival o ml did exampl applic use i-did problem domain particular formul i-did optim prescript followership-leadership multiag tiger problem illustr i-id i-did slightli version multiag tiger problem [ ] problem agent right door or left door ol l addit growl left gl right gr agent also creak left cl right cr creak s noisili other agent s door door tiger origin locat probabl % agent i hear reliabl % creak reliabl % agent j other hand growl reliabl % thu set such agent i agent j door reliabl tiger s growl i j s action indic locat tiger agent s prefer singl agent game [ ] transit observ reward function [ ] good indic use norm method decision-mak i-did emerg realist social behavior prescript set persist multiag tiger problem real world situat followership agent [ ] decept agent follower-lead type relationship particular situat epistemolog condit emerg followership behavior exampl result agent own weak strength prefer possibl behavior other other s action order payoff particular set tiger problem agent i j s prefer own just gold j s hear reliabl comparison exampl j tiger s locat % time % accuraci addit agent i not initi inform tiger s locat other word i s single-level nest belief bi,1 locat tiger addit i consid model j j s flat level initi belief level i-id fig model j probabl tiger left door other sixth intl joint conf autonom agent multi-ag system aama figur level i-id agent i b level id agent j decis node chanc node a1 j a2 j model locat fig b agent i model j i s hear abil correspond level i-id time step norm behavior polici fig exhibit followership behavior i s probabl correctli growl then polici fig i begin condit j s action i same door j previous iff i own assess tiger s locat j s pick i abil correctli growl complet blindli j same door j previous fig b emerg condit followership b blind followership tiger problem behavior interest bold * wildcard observ singl level belief belief other s model suffici followership tiger problem howev epistemolog requir emerg leadership complex agent j leader followership emerg other agent i previous certain prefer ident j j sens hear i j s action time agent j leader i j s belief level leadership role i present j opportun i s action benefit collect good self-interest alon exampl tiger problem set i j correct door then payoff doubl origin j alon correct door payoff other hand agent wrong door penalti half set j s interest as well collect better j expertis correct door thu good leader howev slightli differ problem j gain i s loss i gain specif i payoff j s j antagonist i j correct door wrong then i s loss becom j s gain agent j i incorrectli j s prefer collect good % confid tiger i prefer similar j j start almost sure correct locat level model j i j s action i s norm polici singly-nest i-did time step fig polici i blindli j s action tiger origin locat probabl i same door again j game % probabl tiger right j s i-did level deep result polici fig b even j almost certain ol correct action or ol agent j s intent i j s action so second time step j honest figur emerg decept agent tiger problem behavior interest bold * denot befor agent i s polici blindli j s action b even j almost certain tiger right or ol order i altruism reciproc public good problem public good pg problem ] group m agent resourc public pot resourc public pot agent valuabl agent public pot howev agent resourc then payoff agent one agent share public pot irrespect not action agent not instead free ride other contribut howev behavior human player empir simul pg problem differ norm predict experi mani player initi larg amount public pot pg problem repeatedli amount ] mani experi ] report small core group player persist public pot even other experi also player persist altruist reciproc prefer expect cooper other simplic game m = agent i j agent initi xt amount resourc classic pg game formul agent quantiti resourc ≤ xt public pot action space possibl action agent contribut c fix amount resourc not latter action deth sixth intl joint conf autonom agent multi-ag system aama defect d action not observ other valu resourc public pot ci agent i ci margin privat return ci so agent not public pot privat gain simultan cim > collect contribut pareto optim i/j c d c cixt − cp xt + cjxt − p d xt + cixt − p cjxt − cp xt xt tabl one-shot pg game punish order contribut agent punish free rider small cost punish p punish defect agent non-zero cost contribut agent simplic cost same agent one-shot pg game punish tabl ci = cj cp p > xt − cixt then defect no action p < xt − cixt then defect action p = xt − cixt then game not dominance-solv figur level i-id agent i b level id agent j decis node chanc node a1 j a2 j sequenti version pg problem punish perspect agent i repeat pg game quantiti public pot agent round action formul agent agent fix amount xc defect agent action observ plenti py meager mr state public pot notic observ also indirectli indic agent j s action state public pot amount resourc agent i s privat pot perfectli observ payoff analog tabl empir investig pg problem ] level id j altruist non-altruist type fig b specif altruist agent high margin privat return cj close not punish other xc level agent time action remain type agent action altruist type choos other defect cj altruist type close thu expect punish > − cj altruist type avoid cj non-altruist type not step altruist agent contribut punish > − cj non-altruist type defect step altruist agent public pot close margin privat return non-altruist type prescrib defect decis altruist agent i level i-did time step i level model previous fig i probabl j altruist i choos step behavior persist i unawar j altruist fig i high probabl non-altruist type howev i probabl j non-altruist thu sure defect i choos margin privat return result behavior altruist type resembl experiment non-altruist level agent choos regardless like other agent altruist behavior reciproc agent type cooper defect reciproc type s margin privat return similar non-altruist type howev payoff action similar other case reciproc agent i unsur j altruist public pot like half full prior belief i choos observ plenti i decid observ meager defect fig b observ plenti signal pot like half full result j s action thu model type like altruist like j again next time step agent i choos j s action analog i meager pot action j contribut too punish regardless observ figur altruist level agent alway b reciproc agent i observ plenti j like altruist meager j non-altruist strategi two-play poker poker popular zero sum card game much attent ai research commun testb [ ] poker m player player hand card deck sever flavor poker complex exist simpl version player pli player exchang card e hand k fold f game call c player hand matter simpl m player hand singl card drawn same suit thu showdown player numer card ace pot exchang card discard card either l pile other agent low number card sixth intl joint conf autonom agent multi-ag system aama h pile card rank equal notic exampl number card probabl low card exchang now level i-id simplifi two-play poker fig model person type agent j conserv type like oppon high number card hand other hand aggress agent j high probabl oppon number card thu type belief oppon s hand model oppon action uniform distribut action regardless hand ace aggress agent choos card intent current hand other low card chanc high card exchang conserv agent choos card matter hand chanc high card oppon figur level i-id agent i observ inform j s hand previou time step b level id agent j decis node chanc node a1 j a2 j polici level agent i card own equal likelihood j s hand neutral person type j aggress conserv type fig i s own hand card agent card j not card n i probabl j conserv henc card respond card j equal like card i j card l h i j aggress l i j low card like high card exchang probabl low card high now i choos card h probabl high number card high i choos card final step i choos regardless observ histori belief j card not suffici high payoff partli due fact observ l agent i s previou time step belief j s hand low card onli discuss did i-did enabl onlin sequenti decision-mak uncertain multiag set graphic represent i-did improv previou figur level agent i step polici poker problem i start j equal like aggress conserv card hand equal probabl work significantli transpar semant clear capabl standard algorithm target did i-did extend nid sequenti decision-mak multipl time step presenc other agent i-did concis graphic represent ipomdp way problem structur onlin decision-mak agent act prior belief current way i-did approxim provabl bound solut qualiti acknowledg piotr gmytrasiewicz use discuss work first author support ugarf grant 