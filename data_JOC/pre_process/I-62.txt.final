paper effect stochast resourc alloc problem np-complet complex resourc manag problem qdecomposit approach resourc alreadi agent action agent reward agent q-decomposit reward agent thu permit set state action other hand resourc avail agent qdecomposit possibl heurist search particular bound real-tim dynam program rtdp bound rtdp plan signific state onli action space prune tight upper bound valu function categori subject descriptor i28 [ artifici intellig ] problem solv control method search i211 [ artifici intellig ] artifici intellig gener term algorithm perform experiment introduct paper complex stochast resourc alloc problem gener resourc alloc problem np-complet [ ] such problem schedul process action ie resourc certain task perfectli observ state environ action set task stochast natur problem probabl next visit state gener number state combin possibl specif state task avail resourc case number possibl action state combin individu possibl resourc assign task veri high number state action type problem veri complex mani type resourc alloc problem firstli resourc alreadi agent action agent not state agent global optim polici separ agent second type resourc alloc problem resourc alreadi agent action agent reward agent problem effici qdecomposit russel zimdar ] q-decomposit approach plan agent task agent share limit resourc plan process initi state s0 s0 agent respect q-valu then plan agent arbitr global q-valu respect possibl q-valu agent heurist search number state action optim polici exponenti other known approach q-decomposit first optim heurist search algorithm stochast environ other hand resourc avail agent q-decomposit possibl common way larg stochast problem markov decis process mdp particular real-tim search mani algorithm recent instanc real-tim dynam program rtdp ] lrtdp ] hdp ] ] state-of-the-art heurist search approach stochast environ anytim qualiti interest approach barto al ] state trajectori initi state s0 goal state sg rtdp paper effici resourc alloc problem rtdp much effect action space sub-optim action mcmahan 978-81-904262-7-5 rp c ifaama al ] smith simmon ] singh cohn [ ] stochast problem rtdp type heurist search upper bound valu state mcmahan al ] smith simmon ] particular effici trajectori state updat further converg upper bound effici trajectori state updat approach here paper definit tight bound effici state updat constrain resourc alloc problem other hand approach singh cohn suitabl case paper particular concept margin revenu ] tight bound paper new algorithm upper bound context rtdp heurist search approach margin revenu bound theoret empir bound singh cohn also even algorithm optim polici bound other algorithm mdp onli condit use bound context stochast resourc alloc problem now problem formul a simpl resourc alloc problem follow task ta1 = dish = floor task either realiz state not state task type resourc res1 = brush = deterg comput optim alloc resourc robot task problem state conjunct particular state task avail resourc resourc amount simultan local constraint total global constraint number resourc task expect task reason specif state task chang number avail resourc chang valu state action state s specif state task stochast resourc resourc avail s resourc action resourc consum inde model consum non-consum resourc type consum resourc type amount avail resourc other hand nonconsum resourc type amount avail resourc unchang exampl brush non-consum resourc deterg consum resourc resourc alloc mdp problem transit function reward function markov decis process mdp framework stochast resourc alloc problem mdp wide research today stochast process due fact well-studi simpl yet veri express model world mdp context resourc alloc problem limit resourc tupl re t s a p w r • re = res1 res|res| finit set resourc type avail plan process resourc type local resourc constraint number singl step global resourc constraint number total global constraint onli consum resourc type resc local constraint alway nonconsum resourc type • t finit set task ta ∈ t • s finit set state s ∈ s a state s tupl t res1 res|resc| characterist unaccomplish task ta ∈ t environ avail consum resourc sta specif state task ta also s non empti sg ⊆ s goal state goal state sink state agent forev finit set action assign action ∈ a s applic state combin resourc assign state s particular simpli alloc resourc current task ata resourc alloc ta possibl action lre gre transit probabl pa |s s ∈ s ∈ a s • w = [ wta ] rel weight critic task • state reward r = [ rs ] rsta ← sta × wta rel reward state task rsta product real number weight factor wta problem reward × wta state task sta achiev state other case discount prefer factor γ real number solut mdp polici π state action ∈ a s particular πta s action ie resourc task ta global state s case optim polici expect total reward task optim valu state v s v r a∈a s ∈s pa |s v s consum resourc state s resc \ re consum resourc action inde action resourc assign resc \ new set avail resourc execut action furthermor q-valu q s state action pair sixth intl joint conf autonom agent multi-ag system aama equat q s = r pa |s ∈a q s optim valu state v a∈a q s polici local resourc constraint π lres∀ s ∈ s ∀ ∈ re global constraint system trajectori ∈ t ra system trajectori tra possibl sequenc state-act pair goal state optim polici π exampl state s action possibl system trajectori s s s s global resourc constraint re tra gres∀ tra ∈ t ra ∀ ∈ resc re tra function resourc trajectori tra avail consum resourc state space condit other word model markovian histori not state space furthermor time not model descript also time horizon finit horizon mdp resourc alloc stochast environ np-complet heurist q-decomposit plan problem mani agent comput complex state and/or action space now q-decomposit resourc alloc mani type resourc alloc problem firstli resourc alreadi agent action agent not state agent global optim polici separ agent second type resourc alloc problem resourc alreadi agent action agent reward agent instanc group agent oil countri group agent specif reward right amount oil howev agent agent consum oil pollut exampl type problem interest section naval platform missil ie task resourc ie weapon movement scenario missil type set resourc res1 set resourc res2 type missil rang so case agent set task polici howev interact resourc res1 res2 certain combin resourc not in particular agent i resourc resi first set task t ai agent i resourc resi second set task t ai result polici action not togeth conflict q-decomposit russel zimdar ] context reinforc learn primari assumpt qdecomposit overal reward function r addit separ reward ri distinct agent i ∈ ag |ag| number agent r = i∈ag ri agent valu perspect action other agent i action valu qi ai si state si ∈ si arbitr iter arbitr then action sum agent q-valu global state s ∈ s next time state s agent i valu respect contribut q-valu global maxim q-valu qi ai si valu state such maxa∈a i∈ag qi ai si fact agent determin q-valu valu state extens sarsa on-polici algorithm ] q-decomposit russel zimdar approach local sarsa way ideal compromis agent global optimum inde rather agent successor action agent i action ai arbitr successor state si qi ai si = ri si γ si∈si pai si|si qi ai si consum resourc state si resci \ resi ai resourc alloc problem russel zimdar ] local sarsa converg optimum also case form agent decomposit local q-function much state action space resourc alloc problem briefli section q-decomposit optim solut inde optim bellman backup state algorithm line qdec-backup function agent task respect q-valu here qi ai s optim q-valu agent i state s agent i valu possibl state transit q-valu agent maxim global q-valu state s origin q-decomposit approach brief visit state ∈ s agent respect q-valu respect global state s state space joint state space agent gain complex q-decomposit resid si∈si pai si| part equat agent consid possibl state transit onli possibl state set task number state exponenti number task q-decomposit plan time significantli furthermor action space agent account onli avail resourc much complex standard action space combin possibl resourc alloc state agent then arbitr function line global q-valu sum q-valu agent task line global action case action agent i not simultan action agent i global action simpli action space a s simpli current valu respect global q-valu standard bellman backup then optim polici q-valu agent line sub-act ai specif q-valu qi ai s agent sixth intl joint conf autonom agent multi-ag system aama action a algorithm q-decomposit bellman backup function qdec-backup s v i ∈ ag ai ∈ ai s qi ai s ← ri pai si| qi ai s qi ai s = hi s not yet s \ resi ai consum resourc agent i end end ∈ a s q s i ∈ ag q s ← q s + qi ai s end q s > v then v q s i ∈ ag πi ai qi ai s ← qi ai s end end end standard bellman backup complex o |a| × |sag| |sag| number joint state agent resourc |a| number joint action other hand q-decomposit bellman backup complex o × |ai| × |si | + |a| × |ag| |si| number state agent i resourc |ai| number action agent i |sag| combinatori number task so |si| |s| also |a| combinatori number resourc type resourc alreadi agent number resourc type agent usual set avail resourc type agent circumst |ai| |a| standard bellman backup |a| |sag| much complex |ag| q-decomposit bellman backup thu q-decomposit bellman backup much complex standard bellman backup furthermor commun cost agent arbitr null approach not geograph problem howev resourc avail agent q-decomposit possibl case bound realtim dynam program bounded-rtdp search relev state action space a bound valu state bounded-rtdp now bounded-rtdp bonet geffner ] lrtdp improv [ ] lrtdp simpl dynam program algorithm sequenc trial run start initi state s0 goal solv state lrtdp trial result polici π valu v s bellman backup equat state h s heurist initi valu state s heurist admiss valu heurist underestim optim valu v s object function exampl admiss heurist stochast shortest path problem solut determinist shortest path problem inde problem stochast optim valu determinist version lrtdp admiss initi heurist valu state not loop eventu yield optim valu ] converg mean label procedur procedur solv travers state current trajectori initi state algorithm section bound version rtdp boundedrtdp algorithm action space sub-optim action enabl converg lrtdp bounded-rtdp similar distinct initi heurist unvisit state ∈ s hl s hu s also procedur bound label state hand hl bound valu s such optim valu s hl s part hu upper bound valu s such optim valu s hu s valu bound line bounded-backup function q-valu simultan state transit same q-valu onli valu state transit thu q-valu instead not complex approach fact smith simmon ] state addit time bellman backup bound instead % also particular l s bound state s u s upper bound state s similarli ql s q-valu bound action state s qu s q-valu upper bound action state s bound significantli action space a inde line bounded-backup function qu s ≤ l then action action space s line function state differ upper bound execut back bounded-rtdp function next state line fix number consum resourc avail resc line brief picknextst re none-solv state s reachabl current polici bellman error |u l | final line backup backward fashion visit state trajectori trajectori strategi effici ] ] singh cohn [ ] type algorithm number desir anytim characterist action state s algorithm multipl competit action action bound upper bound state s sixth intl joint conf autonom agent multi-ag system aama bounded-rtdp algorithm [ ] ] function bounded-rtdp s return valu function v repeat s ← ← null repeat visitedpush bounded-backup s resc ← resc \ π s s ← spicknextst resc s goal while = null s ← bounded-backup s end s0 ∀ s ∈ s reachabl s0 return v algorithm bellman backup function bounded-backup s ∈ a s qu s ← r pa |s u s ql s ← r pa |s l l hl s u hu s not yet \ re consum resourc qu s ≤ l then s a s \ re end end l a∈a ql s u a∈a qu s π arg max a∈a ql s |u l then s end far bound optim differ upper bound too high greedi algorithm s choic fast optim solut furthermor new task dynam environ upper bound time arriv singh cohn [ ] algorithm admiss upper bound action space optim solut next section separ method hl s hu s first all method singh cohn [ ] briefli describ then own method bound thu effect prune action space singh cohn s bound singh cohn [ ] upper bound action space approach pretti straightforward first all valu function task standard rtdp approach then task-valu function bound hl upper bound hu particular hl vta sta hu vta sta readabl upper bound singh cohn singhu bound singhl admiss bound singh cohn such upper bound alway optim valu state bound alway optim valu state optim polici π singh cohn algorithm veri similar bounded-rtdp bound l s u s onli differ bounded-rtdp rtdp version singh cohn stop criteria singh cohn algorithm onli competit action state rang competit action state indiffer paramet bounded-rtdp label state |u l < solv converg s0 onli competit action state criteria effect similar one smith simmon ] mcmahan al [ ] paper bound singh cohn bounded-rtdp defin singh-rtdp approach next section bound singh-rtdp effect prune action space upper bound singhu action not possibl resourc constraint upper bound onli possibl action upper bound maxu hu a∈a qta ata sta qta ata sta q-valu task ta state sta action ata standard lrtdp approach upper bound equat admiss proof local resourc constraint upper bound global possibl action howev hu s still v s global resourc constraint not inde task consum resourc own purpos valu task one task global limit resourc maxu bound state complex o |a| × |t a| o |t a| singhu standard bellman backup complex o |a| × |s| |a|×|t a| |a|×|s| comput time upper bound state time visit state much comput time standard bellman backup state usual mani time visit state thu comput time upper bound neglig sixth intl joint conf autonom agent multi-ag system aama idea singhl resourc priori task task own set resourc task independ bound state s hl lowta sta lowta sta valu function task ta ∈ t such resourc priori alloc priori resourc margin revenu highli concept microeconom ] recent coordin decentr mdp ] brief margin revenu extra revenu addit unit product firm thu stochast resourc alloc problem margin revenu resourc addit valu margin revenu resourc task ta state sta follow mrta sta max ata∈a sta qta ata sta max ata∈a sta qta /∈ ata sta concept margin revenu resourc algorithm resourc priori task valu state line algorithm valu function task environ standard lrtdp ] approach valu function also upper bound task avail resourc line valueta variabl variabl estim valu task ta ∈ t begin algorithm resourc specif task thu valueta variabl ta ∈ t then line resourc type re consum non-consum here domain expert avail resourc mani type part resourc order special other word resourc effici small group task earli resourc order qualiti bound line margin revenu consum resourc re task ta ∈ t non-consum resourc resourc not state space other reachabl state sta consid resourc still usabl approach here differ real valu state maxim q-valu state resourc re not state trajectori polici task ta heurist good result other one exampl monte-carlo simul line margin revenu function resourc alreadi task r sgta reward task ta thu vta sta −valueta r sgta residu valu current alloc ta reward task margin revenu term task high residu valu margin revenu high then task ta line margin revenu residu valu line resourc type re group resourc resta task ta afterward line recomalgorithm margin revenu bound algorithm function revenue-bound s return bound lowt ta ∈ t vta ←lrtdp sta valueta end s ← repeat re select resourc type ∈ re ta ∈ t re consum then mrta sta ← vta sta − vta sta re \ re els mrta sta repeat mrta sta ← mrta sta + vta sta max ata∈a sta |res/∈ata qta ata sta sta ← stapicknextst resc sta goal s ← end mrrvta sta ← mrta sta × vta sta −valueta r sgta end ta ← task ta ∈ t maxim mrrvta sta resta ← resta re temp ← re consum then temp ← end valueta ← valueta + vta sta − valueta max ata∈a sta re qta ata sta temp vta sta resourc type ∈ re ta ∈ t lowta ←lrtdp sta resta end return pute valueta first part equat valueta residu valu task ta term max ata∈a sta qta ata sta re vta sta ratio effici resourc type re other word valueta + residu valu valu ratio resourc type re consum resourc q-valu consid onli resourc re state space non-consum resourc resourc avail resourc type manner re empti consum non-consum resourc type task resourc bound compon lowta task line global solut bound follow hl max singhl max a∈a lowta sta maximum singhl bound sum bound compon lowta thu marginalrevenu ≥ singhl particular singhl bound sixth intl joint conf autonom agent multi-ag system aama littl number task compon lowta s0 exampl subsequ state onli task bound singhl lowta compon main differ complex singhl revenue-bound line valu task resourc howev resourc state space action space greatli task greatli calculu valu function line singhl revenue-bound bound equat admiss proof lowta sta resourc lowta sta valu function ta ∈ t not local global resourc constraint inde resourc task not thu hl s realiz polici admiss bound discuss experi domain experi naval platform missil ie task resourc ie weapon movement experi randomli resourc alloc problem approach possibl number task problem |sta| = thu task distinct state type state firstli state action transit probabl then goal state state transit stochast missil state alway mani possibl state particular resourc type probabl missil % % state task missil not state not current state prefer state task effect resourc randomli ±15 % start scenario also local global resourc constraint amount local constraint resourc type task specif state constraint also present real naval platform sensor launcher constraint engag polici furthermor consum resourc total amount avail consum resourc type global constraint randomli start scenario consum resourc type number resourc type consum resourc type non-consum resourc type problem standard lrtdp approach simpl heurist valu unvisit state valu goal state such task way valu unvisit state real valu valu task ta planner ta heurist pretti straightforward advantag heurist evid nevertheless even lrtdp approach simpl heurist still huge part state space not optim polici approach paper figur let approach here • qdec-lrtdp backup qdec-backup function algorithm lrtdp context particular updat checksolv function also qdecbackup function • lrtdp-up upper bound maxu lrtdp • singh-rtdp singhl singhu bound bounded-rtdp • mr-rtdp revenue-bound maxu bound bounded-rtdp qdec-lrtdp set task equal part set task t ai agent i set resourc resi second set task t ai agent agi set resourc resi resi consum resourc type non-consum resourc type resi consum resourc type non-consum resourc type number task odd task t ai constraint group resourc resi resi such assign not possibl constraint arbitr section q-decomposit permit plan time significantli problem set veri effici approach group agent resourc onli avail action agent reward agent bound revenue-bound avail resourc mani type part problem resourc type order special revenue-bound function term experi notic lrtdp lrtdp-up approach resourc alloc not action space much complex instanc averag second lrtdp-up approach task figur singh-rtdp approach plan time upper bound action space mr-rtdp further plan time veri tight initi bound particular singh-rtdp second averag problem task mr-rtdp requir second inde time reduct quit signific lrtdp-up effici bound action space furthermor mr-rtdp singhu bound slightli effici maxu bound also mr-rtdp singhl bound slightli effici singh-rtdp result differ effici mr-rtdp singh-rtdp attribut marginal-revenu bound maxu upper bound inde number task high bound singh-rtdp valu singl task other hand bound mr-rtdp account valu sixth intl joint conf autonom agent multi-ag system aama timeinsecond number task lrtdp qdec-lrtdp figur effici q-decomposit lrtdp lrtdp timeinsecond number task lrtdp lrtdp-up singh-rtdp mr-rtdp figur effici mr-rtdp singh-rtdp task heurist resourc inde optim alloc resourc way task bound heurist experi q-decomposit veri effici approach group agent resourc onli avail action agent reward agent other hand avail resourc q-decomposit possibl tight bound heurist search case plan time bounded-rtdp action space significantli lrtdp furthermor margin revenu bound paper favor singh cohn [ ] approach boundedrtdp bound wide rang stochast environ onli condit use bound task consum and/or non-consum limit resourc interest research avenu bound other heurist search algorithm instanc frtdp ] [ ] effici heurist search algorithm particular approach effici state trajectori updat upper bound tight bound frtdp brtdp number backup converg final bounded-rtdp function action space qu s ≤ l s singh cohn [ ] frtdp brtdp also action space circumst plan time 