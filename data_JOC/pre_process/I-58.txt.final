adversari multiag domain secur commonli abil intent threat other agent critic issu paper domain threat unknown adversari domain bayesian game much work equilibria such game howev often case multiag secur agent mix strategi adversari own strategi case agent reward optim strategi equilibrium previou work problem optim strategi select np-hard therefor heurist asap key advantag problem first asap highest-reward strategi rather bayes-nash equilibrium feasibl strategi natur first-mov advantag game second strategi simpl repres implement third directli compact bayesian game represent convers normal form effici mix integ linear program milp implement asap experiment result signific speedup reward other approach categori subject descriptor i211 [ comput methodolog ] artifici intellig artifici intellig intellig agent gener term secur design theori introduct mani multiag domain agent order secur attack adversari common issu agent such secur domain uncertainti adversari exampl secur robot choic area often ] howev not advanc exactli robber team unman aerial vehicl uav ] region humanitarian crisi also patrol polici decis advanc terrorist other adversari mission locat inde possibl motiv type adversari agent agent team like order adversari close howev case secur robot uav team not exactli kind adversari activ day common approach polici agent such scenario scenario bayesian game bayesian game game agent type type agent possibl action payoff distribut adversari type agent histor data usual game solut concept bayes-nash equilibrium extens nash equilibrium bayesian game howev mani set nash bayes-nash equilibrium not appropri solut concept agent strategi simultan [ ] set player commit strategi other player strategi scenario stackelberg game ] stackelberg game leader strategi first then follow group follow selfishli own reward action leader exampl secur agent leader first strategi variou area strategi mix strategi order unpredict robber follow robber pattern patrol time then strategi locat leader stackelberg game reward strategi simultan advantag leader stackelberg game simpl game payoff tabl tabl leader row player follow column player here leader payoff first tabl payoff tabl exampl normal form game onli nash equilibrium game leader follow leader payoff 978-81-904262-7-5 rp c ifaama howev leader uniform mix strategi equal probabl follow s respons payoff equal probabl leader payoff then equal probabl case leader now incent pure strategi payoff howev follow strategi as well nash equilibrium thu strategi follow temptat leader reward nash equilibrium problem optim strategi leader stackelberg game [ ] np-hard case bayesian game multipl type follow thu effici heurist techniqu highreward strategi game import open issu method optim leader strategi non-bayesian game ] problem bayesian game normal-form game harsanyi transform ] other hand highest-reward nash equilibrium new method mixed-integ linear program milp ] highest-reward bayes-nash equilibrium equival correspond nash equilibrium transform game howev game compact structur bayesian game addit nash equilibrium simultan choic strategi advantag leader not paper effici heurist method optim leader strategi secur domain asap agent secur approxim polici method key advantag first directli optim strategi rather nash bayes-nash equilibrium thu high-reward non-equilibrium strategi abov exampl second polici support uniform distribut multiset fix size [ ] polici simpl [ ] as well tunabl paramet size multiset simplic polici third method bayes-nash game compactli convers normal-form game larg speedup nash method such [ ] ] rest paper section fulli patrol domain properti section bayesian game harsanyi transform method optim leader s strategi stackelberg game then section asap algorithm normal-form game section structur bayesian game uncertain adversari experiment result reward faster polici comput nash method section discuss relat work section patrol domain secur patrol domain secur agent uav [ ] secur robot ] not patrol area time instead polici variou rout differ time account factor such likelihood crime differ area possibl target crime secur agent own resourc number secur agent amount avail time fuel etc usual benefici polici nondeterminist robber not safe rob certain locat safe secur agent ] util algorithm simplifi version such domain game basic version game player secur agent leader robber follow world consist m hous secur agent s set pure strategi possibl rout d hous order secur agent mix strategi robber unsur exactli secur agent robber mix strategi secur agent exampl robber time often secur agent area knowledg robber singl hous robber gener long time hous hous robber not secur agent s rout then robber success otherwis secur agent s rout then hous rout secur agent robber payoff game follow variabl • vl x valu good hous l secur agent • vl q valu good hous l robber • cx reward secur agent robber • cq cost robber • pl probabl secur agent robber lth hous patrol pl < pl ⇐⇒ l < l secur agent s set possibl pure strategi rout x d-tupl < w1 w2 > w1 wd = m element equal agent not same hous robber s set possibl pure strategi hous q integ = payoff secur agent robber pure strategi j • −vl x vl q j = l /∈ i plcx + x −plcq + q j = l ∈ i structur possibl mani differ type robber motiv exampl robber cost valu good variou hous differ distribut differ robber type histor data then game bayesian game ] bayesian game a bayesian game set n agent agent n set type θn patrol domain agent secur agent robber θ1 set secur agent type θ2 set robber type onli type secur agent θ1 onli element game robber type secur agent not robber s type agent secur agent robber n set strategi σn util function un θ1 × θ2 × σ1 × σ2 → a bayesian game normal-form game harsanyi transform ] onc new linear-program lp method high-reward strategi normal-form game ] strategi transform game strategi then bayesian game method bayes-nash equilibria directli harsanyi transform ] onli sixth intl joint conf autonom agent multi-ag system aama singl equilibrium gener case not high reward recent work [ ] mixed-integ linear program techniqu nash equilibrium agent howev techniqu normal-form game so polici asap optim polici as well highest-reward nash equilibrium techniqu harsanyi-transform matrix next subsect harsanyi transform first step bayesian game harsanyi transform ] bayesian game normal form game harsanyi transform standard concept game theori simpl exampl patrol domain mathemat formul type b bayesian game robber activ probabl α robber b activ probabl − α rule section allow simpl payoff tabl hous world henc patrol rout pure strategi agent robber hous hous henc strategi robber type l type b payoff tabl tabl correspond secur agent separ game type probabl α − α first robber type notat domain section valu variabl v1 x = v1 q = v2 x = v2 q = cx cq p1 p2 valu base payoff tabl payoff game robber type exampl secur agent rout activ choos hous robber reward -1 agent reward robber payoff game robber type b differ valu secur agent robber -1 5 -375 -125 -125 -1 5 robber -9 6 -275 -025 -025 -9 6 tabl payoff tabl secur agent vs robber b harsanyi techniqu chanc node robber s type thu secur agent incomplet inform robber imperfect inform ] bayesian equilibrium game then precis nash equilibrium imperfect inform game transform normal-form game tabl transform game secur agent column player set type togeth row player robber type rob hous type b rob hous secur agent patrol then secur agent robber payoff payoff agent hous probabl α b hous probabl − α optim strategi nash equilibrium standard solut concept game agent strategi simultan secur domain secur agent leader advantag mix strategi advanc follow robber leader s strategi optim respons follow pure strategi common assumpt [ ] case follow indiffer strategi benefit leader guarante optim strategi leader ] bayesian game tabl harsanyi bimatrix tabl strategi player secur agent robber transform game correspond combin possibl strategi player s type therefor x = σθ1 = σ1 q = index set secur agent robber pure strategi respect r c payoff matric rij reward secur agent cij reward robber secur agent pure strategi i robber pure strategi j mix strategi secur agent probabl distribut set pure strategi vector x = px1 px2 px|x| pxi p here pxi probabl secur agent ith pure strategi optim mix strategi secur agent time polynomi number row normal form game follow linear program formul ] possibl pure strategi j follow set robber type max p i∈x pxirij st ∈ q p pxicij ≥ p i∈σ1 pxicij p i∈x pxi ∀i∈x pxi > then feasibl follow strategi one p i∈x pxirij reward secur agent leader pxi variabl optim strategi secur agent note method polynomi number row transform normal-form game number row increas exponenti number robber type method bayesian game thu |σ2||θ2| separ linear program surpris leader s optim strategi bayesian stackelberg game np-hard [ ] heurist approach given optim strategi leader np-hard heurist approach heurist possibl mix strategi leader action probabl integ multipl predetermin integ previou work [ ] strategi high entropi benefici secur applic oppon util complet unknown domain util not method uniform-distribut strategi advantag such strategi compact fraction simpl effici real organ advantag simpl strategi secur applic problem effect robber reward secur agent s reward thu asap heurist strategi k-uniform mix strategi k-uniform uniform distribut multiset s pure strategi |s| = multiset set element multipl time thu exampl mix strategi multiset strategi sixth intl joint conf autonom agent multi-ag system aama −1α − 9 − α 5α + 6 − α − 275 − α 125α + 225 − α −1α − 025 − α 5α − 025 − α − 9 − α 125α + 6 − α −125α − 9 − α −125α + 6 − α − 275 − α 5α + 225 − α −125α − 025 − α −125α − 025 − α − 9 − α 5α + 6 − α harsanyi transform payoff tabl probabl strategi probabl asap size multiset order complex strategi goal strategi high reward advantag asap heurist directli compact bayesian represent harsanyi transform differ follow robber type independ other henc leader strategi harsanyi-transform game matrix equival game matric individu follow type independ properti asap decomposit scheme note lp method [ ] optim stackelberg polici unlik decompos small number game np-hard bayes-nash problem final note asap solut onli optim problem rather seri problem lp method ] singl follow type algorithm follow way particular k possibl mix strategi x leader multiset size k leader payoff x follow reward-maxim pure strategi then mix strategi payoff onli reward-maxim pure strategi follow robber strategi x secur agent robber type problem fix linear reward mix strategi optim robber then so pure strategi support mix strategi note also that leader s strategi discret valu assumpt section follow tie leader s favor not signific tie unlik becaus domain reward random distribut probabl pure optim respons leader strategi leader onli finit number possibl mix strategi approach optim strategi secur agent use properti linear program outlin result here complet detail discuss proof mani refer topic such [ ] linear program problem such max ct x | ax = b x ≥ dual linear program case min bt y | at y ≥ primal/du pair problem weak dualiti x y primal dual feasibl solut respect ct x ≤ bt y pair feasibl solut optim ct x = bt y problem strong dualiti fact linear program feasibl optim solut then dual also feasibl pair x∗ y∗ x∗ = bt y∗ optim solut follow optim condit [ ] • primal feasibl ax = b x • dual feasibl at y ≥ • complementari slack xi at y − c i i note last condit impli x = xt at y = bt y optim primal dual feasibl solut follow subsect first problem intuitit form mixed-integ quadrat program miqp then problem mixedinteg linear program milp mixed-integ quadrat program case singl type follow leader row player column player vector strategi leader vector strategi follow also x q index set leader follow s pure strategi respect payoff matric r c correspond rij reward leader cij reward follow leader pure strategi i follow pure strategi j k size multiset first polici leader k-uniform polici x valu xi number time pure strategi i k-uniform polici probabl xi/k optim problem solv optim respons follow linear program max x j∈q x k cijxi qj st p j∈q qj q object function follow s reward x constraint feasibl mix strategi q follow dual linear program problem follow st ≥ x i∈x k cijxi j ∈ q strong dualiti complementari slack follow s reward valu valu pure strategi qj > support optim mix strategi pure strategi optim optim solut follow s problem linear optim condit primal feasibl constraint dual feasibl constraint complementari slack − x i∈x k cijxi j ∈ q condit problem leader order onli respons follow k-uniform polici x sixth intl joint conf autonom agent multi-ag system aama leader k-uniform solut x own payoff follow optim respons q x leader follow integ problem max x i∈x x j∈q k rijq x j xi st p i∈x xi = k xi ∈ k problem leader reward follow s respons qj fix leader s polici x henc q x j uniform polici multiset constant size k problem character q x linear program optim condit complementari slack condit q x onli optim pure strategi just integ solut q x leader s problem maxx q x i∈x x j∈q k rijxiqj st p i = kp j∈q qj ≤ − p k cijxi ≤ − qj m xi ∈  k qj ∈ here constant m larg number first fourth constraint k-uniform polici leader second fifth constraint feasibl pure strategi follow third constraint enforc dual feasibl follow s problem leftmost inequ complementari slack constraint optim pure strategi q follow rightmost inequ fact onli pure strategi follow qh = last constraint enforc = p i∈x k cihxi addit constraint other pure strategi = subsect problem integ program non-convex quadrat object gener matrix r not positive-semi-definit effici solut method non-linear non-convex integ problem challeng research question next section reformul problem linear integ program problem number effici commerci solver mixed-integ linear quadrat program problem chang variabl = xiqj problem maxq z p i∈x p j∈q k rijzij st p i∈x p j∈q zij = k p j∈q zij ≤ k kqj ≤ p i∈x zij ≤ k p j∈q qj ≤ − p k cij p h∈q zih ≤ − qj m zij ∈  k qj ∈ proposit problem equival proof consid x feasibl solut q zij = xiqj feasibl solut same object function valu equival object function constraint construct fact p j∈q zij = xi p j∈q qj explain constraint satisfi p i∈x zij = kqj now q z feasibl q xi = p j∈q zij feasibl same object valu fact constraint readili construct object qh then third constraint impli p zih = k zij i ∈ x j = h therefor xiqj = x = zihqj = zij last equal j = h transform object function valu proof transform mixed-integ linear program milp now decomposit techniqu milp signific speedup bayesian game multipl follow type decomposit multipl adversari milp previou section onli follow secur scenario multipl follow robber type respons function follow pure strategi weight combin variou pure follow strategi weight probabl occurr follow type miqp multipl adversari framework notat previou section reason multipl follow type vector strategi leader vector strategi l l index set type also x q index set leader l s pure strategi respect also index payoff matric follow l matric rl cl notat optim solut l s problem leader k-uniform polici x follow optim condit x j∈q ql j al − x k cl ijxi ql j al − x k cl ijxi ql j again onli optim pure strategi l s problem complementar constraint constraint leader s problem optim k-uniform polici therefor priori probabl l ∈ l follow leader follow problem sixth intl joint conf autonom agent multi-ag system aama maxx q x i∈x x x j∈q pl k rl ijxiql j st p i = kp j∈q ql j ≤ al − p k cl ijxi ≤ − ql j m xi ∈  k j ∈ problem bayesian game multipl type inde equival problem payoff matrix harsanyi transform game fact pure strategi j problem correspond sequenc pure strategi jl l ∈ l qj onli ql jl l ∈ l addit priori probabl pl player l reward harsanyi transform payoff tabl rij = p pl rl ijl same relat c cl relat pure strategi equival normal form game pure strategi individu game follow key problem equival decompos milp quadrat program problem chang variabl ij = xiql j problem maxq z p i∈x p p j∈q pl k rl ijzl ij st p i∈x p j∈q zl ij = k p j∈q zl ij ≤ k kql j ≤ p i∈x zl ij ≤ k p j∈q ql j ≤ al − p k cl ij p zl ih ≤ − ql j m p j∈q zl ij = p j∈q z1 ij zl ij ∈  k j ∈ proposit problem equival proof consid x ql al l ∈ l feasibl solut ql al zl ij = xiql j feasibl solut same object function valu equival object function constraint construct fact p j∈q zl ij = xi p j∈q ql j explain constraint satisfi p i∈x zl ij = kql j let now ql zl feasibl ql al xi = p j∈q z1 ij feasibl same object valu fact constraint readili construct object notic l ql j rest equal ql jl then third constraint impli p zl ijl = k zl ij i ∈ x j = jl particular xi = x j∈q z1 ij = z1 ij1 = zl ijl last equal constraint therefor xiql j = zl ijl ql j = zl ij last equal j = jl effect constraint ensur adversari respons particular fix polici agent transform object function valu proof equival linear integ program effici integ program packag problem thousand integ variabl decompos milp result follow section experiment result patrol domain payoff associ game section experi game world hous patrol hous descript section base case secur agent payoff function payoff tabl addit robber type game random distribut size payoff base case game so robber type minimum maximum payoff secur agent robber respect data experi method secur agent s strategi • uniform random • asap multipl linear program ] true optim strategi reward bayes-nash equilibrium mip-nash algorithm last method cplex last method normal-form game rather bayesian game game first harsanyi transform ] uniform random method simpli uniform random polici possibl patrol rout method simpl baselin perform heurist uniform polici reason well maximum-entropi polici effect multiag secur ] highest-reward bayes-nash equilibria order reward optim polici rather equilibria stackelberg game such secur domain experi set graph runtim asap other common method strategi reward asap other method effect paramet k size multiset perform asap first set graph asap multiset element third set number first set graph figur runtim graph three-hous left column four-hous right column row graph correspond differ randomly-gener scenario x-axi number robber type secur agent y-axi graph runtim second experi not minut second runtim uniform polici alway neglig irrespect number adversari henc not asap clearli optim multiplelp method as well mip-nash algorithm highestreward bayes-nash equilibrium respect sixth intl joint conf autonom agent multi-ag system aama figur runtim variou algorithm problem hous domain hous optim method not solut robber type hous not type cutoff time scenario mip-nash solv even robber type cutoff time other hand asap much abl adversari three-hous scenario adversari four-hous scenario cutoff time runtim asap not strictli number robber type scenario gener addit type runtim second set graph figur reward patrol agent method scenario three-hous left column four-hous right column reward util secur agent patrol game not percentag optim reward not possibl optim reward number robber type uniform polici consist reward domain optim method cours optim reward asap method consist close optim even number robber type increas highest-reward bayes-nash equilibria mipnash method reward uniform method asap differ clearli gain patrol domain strategi leader stackelberg game rather standard bayes-nash strategi third set graph figur effect multiset size runtim second left column reward right column again reward secur agent patrol game not percentag optim figur reward variou algorithm problem hous reward result here three-hous domain trend that as multiset size runtim reward level increas not surprisingli reward monoton multiset size increas rel littl benefit larg multiset domain case reward multiset element % reward 80-element multiset runtim not alway strictli multiset size inde exampl scenario robber type multiset element second element onli second gener runtim multiset domain variabl milp thu search space howev increas number variabl sometim polici quickli due flexibl problem summari and relat work paper secur agent hostil environ environ intent threat adversari secur agent incomplet inform specif situat adversari action payoff exact adversari type unknown secur agent agent real world quit frequent such incomplet inform other agent bayesian game popular choic such incomplet inform game ] gala toolkit method such game ] game normal form harsanyi transform ] gala s guarante fulli competit game much work optim bayes-nash equilbria sixth intl joint conf autonom agent multi-ag system aama figur reward asap multiset element subclass bayesian game singl bayes-nash equilibria gener bayesian game ] approxim bayes-nash equilibria ] less attent optim strategi bayesian game stackelberg scenario ] howev complex problem np-hard gener case ] also algorithm problem non-bayesian case therefor heurist asap key advantag problem first asap reward strategi rather bayes-nash equilibrium feasibl strategi natur first-mov advantag game second strategi simpl repres implement third directli compact bayesian game represent convers normal form effici mix integ linear program milp implement asap experiment result signific speedup reward other approach k-uniform strategi similar k-uniform strategi [ ] work epsilon error-bound k-uniform strategi solut concept still nash equilibrium not effici algorithm such k-uniform strategi asap emphasi highli effici heurist approach not equilibrium solut final patrol problem work recent attent multiag commun due wide rang applic ] howev work energi consumpt [ ] criteria length path [ ] explicit model adversari [ ] acknowledg research unit state depart homeland secur center risk econom analysi terror event creat also defens advanc research project agenc darpa depart interior nbc acquisit servic divis contract no nbchd030010 sarit krau also umiac 