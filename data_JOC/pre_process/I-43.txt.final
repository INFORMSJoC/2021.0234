paper dynam control dbc approach plan control agent stochast environ approach expect reward eg partial observ markov decis problem pomdp dbc system behavior toward system dynam recent plan control approach extend markov track emt instanti dbc emt greedi action select effici control algorithm markovian environ effici set experi emt class area-sweep problem target such problem natur effici dbc framework emt instanti categori subject descriptor i28 [ problem solv control method search ] control theori i29 [ robot ] i211 [ distribut artifici intellig ] intellig agent gener term algorithm theori introduct plan central research area multiag system artifici intellig recent year partial observ markov decis process pomdp ] popular formal basi stochast environ framework plan control problem often reward function polici action optim sens expect util theoret attract complex optim pomdp prohibit [ ] altern view plan stochast environ not state-bas reward function instead differ criterion transition-bas specif desir system dynam idea here planexecut process stochast system plan dynam process chang criteria gener plan framework dynam control dbc dbc goal plan control process system accord specif potenti stochast target dynam actual system behavior target dynam due stochast natur system such environ need continu [ ] manner similar classic closed-loop control ] here optim term probabl deviat magnitud paper structur dynam control recent extend markov track emt approach ] dbc emt greedi action select specif parameter option possibl dbc emt effici instanti dbc dbc set experi multi-target emt tag game [ ] variant area sweep problem agent target quarri posit not certainti experiment data even simpl model environ simpl design target dynam high success rate quarri quarri observ entropi agent s posit paper section dbc area-sweep problem relat work section dynam control dbc structur special markovian environ review extend markov track emt approach dbc-structur control regimen section section also limit emt-bas control rel gener dbc framework experiment set result then section section short discuss overal approach section conclud remark direct futur work 978-81-904262-7-5 rp c ifaama motiv and relat work mani real-lif scenario natur stochast target dynam specif especi domain ultim goal rather system behavior specif properti continu exampl secur guard persist sweep area sign intrus thiev sweep time oper point guard motion thu advis guard motion dynam irregular random recent work paruchuri al ] such random context single-ag distribut pomdp goal work polici measur action-select random reward accept level focu differ work dbc not expect rewards-inde not reward all-but instead maintain dynam not limit random game tag exampl applic approach work pineau al ] agent area grid grid cell hole agent agent hunter cell other quarri such co-loc success tag quarri hunter agent alway awar hunter s posit not hunter possibl hunter prey hunter quarri probabilist law motion not current locat tag instanc famili area-sweep pursuit-evas problem [ ] hunter problem pomdp a reward function desir quarri action polici reward time due intract complex optim polici action polici paper essenti approxim paper instead reward function emt problem directli target dynam fact search problem random motion socal class area problem specif such target system dynam dynam control natur approach problem dynam base control specif dynam control dbc level environ design level user level agent level environ design level formal specif model environ exampl level law physic system paramet such gravit constant user level turn reli environ model environ design target system dynam user level also estim learn procedur system dynam measur deviat museum guard scenario stochast sweep schedul measur rel surpris actual sweep • agent level turn combin environ model environ design level dynam estim procedur deviat measur target dynam specif user level sequenc action system dynam as close possibl target specif interest continu develop stochast system such happen classic control theori ] continu plan ] as well exampl museum sweep question agent level deviat measur time end probabl threshold-that agent level probabl deviat measur certain threshold specif action select then system formal possibl mixtur avail system trend much behavior-bas robot architectur ] other altern estim procedur user level-to environ design level model environ action so dynam certain dynam notic manipul not direct environ thu strong enough estim algorithm success manipul success simul target dynam ie avail sensori input dbc level also back-flow inform figur instanc agent level data target dynam feasibl user level requir perhap attain featur system behavior data also avail system respons differ action dynam estim user level import tool environ model calibr environ design level userenv design agent model ideal dynam estim estim dynam feasibl system respons data figur data flow dbc framework extend idea actor-crit algorithm ] dbc data flow good basi design learn algorithm exampl user level exploratori devic learn algorithm ideal dynam target environ model hand critic featur system behavior case feasibl system respons data agent level key inform environ model updat fact combin feasibl respons data basi applic strong algorithm such em [ ] dbc markovian environ partial observ markovian environ dbc rigor manner notic dbc reward replac optim criterion structur differ tabl • environ design level tupl < s a t o ω s0 > s set possibl environ state s0 initi state environ also probabl distribut s sixth intl joint conf autonom agent multi-ag system aama set possibl action applic environ t environ probabilist transit function t s ×a → π s t s |a s probabl environ state s state s action o set possibl observ sensor input outsid observ ω observ probabl function ω s a × s → π o ω s probabl environ state s state s action • user level case markovian environ set system dynam famili condit probabl f = τ s a → π s thu specif target dynam q ∈ f learn algorithm function l o× a×o → f sequenc observ action so far estim τ ∈ f system dynam mani possibl variat avail user level diverg system dynam sever trace distanc l1 distanc distribut p q d p · · x |p x q x fidel measur distanc f p · · x p x q x kullback-leibl diverg dkl p · q · x p x log p x q x notic latter not actual metric space possibl distribut nevertheless meaning import interpret instanc kullbackleibl diverg import tool inform theori ] price inform sourc q user level also threshold dynam deviat probabl θ agent level then problem control signal function a∗ minim problem a∗ = pr d q > θ d q random variabl deviat dynam τa l control signal ideal dynam implicit minim problem l environ environ model environ design level dbc view state space import complementari view dbc pomdp state space environ pomdp regard state stationari snap-shot environ attribut state seek properti control process case reward accumul sequenc state attribut onli mechanism-th pomdp polici dbc underli principl state sequenc system dynam dbc s target dynam specif environ s state space mean discern preserv chang system result dbc abil state sequenc properti environ model state space definit exampl task rough terrain goal as fast possibl pomdp terrain state space point speed neg reward step goalaccumul reward onli motion altern state space directli notion speed pomdp same concept twice sens directli state space indirectli reward accumul now even reward function finer detail properti motion pomdp solut much space polici still implicit concept reward accumul procedur other hand tactic target express variat correl posit speed motion now state space represent situat further constraint eg smooth motion speed limit differ locat speed reduct sharp turn explicitli uniformli tactic target faster effect action select dbc algorithm emt-bas control a dbc recent control algorithm emt-bas control [ ] dbc framework approxim greedi solut dbc sens initi experi emt-bas control [ ] emt-bas control markovian environ definit case pomdp user agent level markovian dbc type optim user level emt-bas control limited-cas target system dynam independ action qemt s → π s then kullback-leibl diverg measur momentari system dynam estimator-th extend markov track emt algorithm algorithm system dynam estim τt emt capabl recent chang auxiliari bayesian system state estim pt−1 conserv kullback-leibl diverg τt emt pt−1 respect condit margin probabl system s state space explan simpli pt s τt emt s |s pt−1 s dynam updat sixth intl joint conf autonom agent multi-ag system aama tabl structur pomdp dynamics-bas control markovian environ level approach mdp markovian dbc environ < s a t o ω > s set state a set action design t s a → π s transit o observ ω s a × s → π o user r s a × s → r q s a → π s f π∗ r l o1 reward function ideal dynam f reward l dynam threshold agent π∗ = π e [ γt rt ] π∗ = prob d τ q > θ minim problem τt emt = h [ pt pt−1 τt−1 emt ] = dkl τ × pt−1 τt−1 emt × pt−1 st pt s τ × pt−1 s pt−1 s τ × pt−1 s agent level emt-bas control suboptim respect dbc dbc framework greedi action select predict emt s reaction predict environ model environ design level ta environ s transit function action pt−1 auxiliari bayesian system state estim then emt-bas control choic a∗ = arg min a∈a dkl h [ ta × pt pt τt emt ] qemt × pt−1 note markovian dbc framework precis optim pomdp place dynam estim emt case action effect environ estim close target system dynam naiv control suboptim dbc sens sever addit limit not gener dbc framework section multi-target emt time sever behavior prefer exampl case museum guard art item heavili guard often close vicin other hand corner museum uncheck constant motion success museum secur guard adher balanc behavior emt-bas control sever tactic target qk k k=1 question mechan issu emt-bas control action prefer vector set action perform respect target prefer vector singl unifi prefer replac standard emt-bas action select algorithm ] • given set target dynam qk k k=1 vector weight w k • select action action ∈ a predict futur state distribut ¯pa t+1 = ta ∗ pt action comput da = h ¯pa t+1 pt pdt a qk tactic target denot v k = dkl da qk pt vk zk v k zk = a∈a v k normal factor select a∗ = k k=1 w k vk weight vector w = w1 wk addit tune import target dynam need target method also seamlessli emt-bas control flow oper emt-bas control limit emt-bas control sub-optim dbc sens repres dbc structur user emt dynam track algorithm replac agent optim greedi action select kind combin howev common on-lin algorithm further develop emt-bas control necessari evid so far even form algorithm great deal power display trend optim dbc sens word further emt-specif limit emt-bas control evid point alreadi partial solut subject research first limit problem neg prefer pomdp framework exampl simpli sixth intl joint conf autonom agent multi-ag system aama appear valu differ sign reward structur emt-bas control howev neg prefer certain distribut system develop sequenc emt-bas control howev concentr as close possibl distribut avoid thu unnatur nativ emt-bas control second limit fact standard environ model pure sensori actions-act not state world onli way observ qualiti observ world state not emt-bas control not abl differ sensori action notic limit emt-bas control gener dbc framework track algorithm capabl pure sensori action kullback-leibl diverg distribut deviat measur capabl neg prefer emt play tag game tag first [ ] singl agent problem quarri belong class area problem exampl domain figur figur tag domain agent quarri q game tag extrem agent s percept agent abl quarri onli co-loc same cell grid world classic version game co-loc lead special observ ‘ tag action slightli set moment agent same cell game result agent quarri same motion capabl direct north south east west formal space action markovian environ state space formal markovian environ cross-product agent quarri s posit figur s = s0 × s0 s23 effect action agent determinist environ gener stochast respons due motion quarri probabl probabl − q0 adjac cell further away experi q0 = agent so instanc figur = p q = = s9 a = s11 p q = = s9 a = s11 p q = = s9 a = s11 p q = = s9 a = s11 evas behavior quarri agent quarri s posit not onli sensori inform avail agent own locat emt directli target dynam game tag easili major trend quarri mobil quarri result follow target dynam tcatch at+1 = = sj = sa si = otherwis tmobil at+1 = si|qt = so = sj si = otherwis tstalk at+1 = si|qt = so = sj dist si so none abov target directli achiev instanc qt = s9 = s11 action agent at+1 = tcatch target dynam sever experi emt perform tag game configur domain figur differ challeng agent due partial observ set set run time limit step run initi posit agent quarri random as far agent quarri initi posit uniformli entir domain cell space also variat environ observ function first version observ function joint posit hunter quarri posit hunter observ second onli joint posit hunter quarri differ locat hunter s locat second version fact util fact onc same cell game result experi tabl [ catch move target dynam previou section weight vector ] emt stabl perform domain direct comparison difficult emt perform notabl effici vis- a-vi pomdp approach spite simpl ineffici matlab implement emt algorithm decis time step significantli second experi irregular open arena domain difficult experi run step total step slightli hour × onlin step order magnitud time offlin comput pomdp polici ] signific differenti even promin fact environ model paramet onlin natur emt perform time pomdp polici yet again larg overhead comput also behavior cell frequenc entropi empir measur trial data figur figur show sixth intl joint conf autonom agent multi-ag system aama q q a a q figur configur tag game space multipl dead-end b irregular open arena c circular corridor tabl perform emt-bas solut tag game domain observ model omniposit quarri ii quarri not hunter s posit model domain captur % e step time/step msec arena msec circl msec ii msec arena msec circl msec ical entropi grow length interact run quarri not immedi entropi differ run scenario agent activ quarri entropi never maximum characterist entropi graph open arena scenario particularli attent case omniposit quarri observ model maximum limit trial length step suddenli further analysi data certain circumst fluctuat behavior occur agent equal viabl version quarry-follow behavior emt algorithm action select state space not form commit not even acceler agent small portion cell essenti simultan sever cours action consist target dynam behavior not second observ model significantli set elig cours action-essenti tie-break design emt solut tag game core differ approach plan control emt dbc hand familiar pomdp approach other pomdp reward structur influenc system dynam indirectli optim emt reward scheme instead measur influenc system dynam directli entropi log base equal number possibl locat domain properli scale express rang ] domain thu tag game not reward function prefer agent s behavior rather directli heurist behavior prefer basi target dynam experiment data show target not directli achiev agent s action howev ratio emt perform achiev target dynam tag game experi data also differ emphasi dbc pomdp place formul environ state space pomdp reli entir mechan reward accumul maxim format action select procedur necessari state sequenc dbc other hand sourc specif properti action select procedur direct specif target dynam import second sourc tag game experi data greedi emt algorithm pomdp-typ state space specif target descript state space incap necessari behavior tendenc eg tie-break commit motion structur differ dbc emt particular pomdp direct perform comparison complementari track suitabl nich instanc pomdp much natur formul econom sequenti decision-mak problem emt fit continu demand stochast chang happen mani robot embodied-ag problem complementari properti pomdp emt further recent interest pomdp hybrid solut ] pomdp togeth other control approach result not easili achiev approach effect partner such hybrid solut instanc pomdp prohibit larg off-lin time requir polici comput readili simpler set benefici behavior trend form target dynam emt domain on-lin oper conclus futur work paper novel perspect process plan control stochast environ form dynam control dbc framework dbc task plan support target system dynam necessari properti chang environ optim dbc plan action sixth intl joint conf autonom agent multi-ag system aama step entropi step entropi arena step entropi circl figur observ model omniposit quarri entropi develop length tag game experi scenario multipl dead-end b irregular open arena c circular corridor step entropi step entropi arena step entropi circl figur observ model ii quarri not hunter s posit entropi develop length tag game experi scenario multipl dead-end b irregular open arena c circular corridor respect deviat actual system dynam target dynam recent techniqu extend markov track emt ] instanti dbc fact emt specif case dbc parameter greedi action select procedur emt key featur gener dbc framework as well polynomi time complex multitarget version emt [ ] class area problem natur dynamics-bas descript experi tag game domain section emt number limit such difficulti neg dynam prefer direct applic emt such problem rendezvous-evas game eg ] howev dbc gener such limit readili formul evas game futur work develop dynamics-bas control problem work first author partial israel scienc foundat grant third author partial grant israel s ministri scienc technolog 